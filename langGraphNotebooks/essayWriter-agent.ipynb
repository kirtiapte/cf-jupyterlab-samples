{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e741c4-bf73-4f95-8265-8e85ecf75e1f",
   "metadata": {},
   "source": [
    "## Langgraph Essay Writer Agent \n",
    "\n",
    "You are an expert writer tasked with writing a high level outline of an essay.  Write such an outline for user provided topic.  Give an outline of the essay along with notes and instructions for the sections\n",
    "\n",
    "##### Note: Go to JupyterLab terminal and execute following command before getting started\n",
    "<pre>\n",
    "    uv add langgraph\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316b0f92-e9cd-49d2-85fe-08a803e29513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': '### **Essay Outline: Understanding the Difference Between LangChain and LangSmith**\\n\\n---\\n\\n#### **I. Introduction**  \\n- **Purpose**: Introduce the growing importance of language model frameworks in AI development.  \\n- **Thesis**: While LangChain is a framework for building language model applications, LangSmith is a platform for developing, testing, and deploying these applications.  \\n- **Relevance**: Highlight how both tools complement each other in the AI ecosystem.\\n\\n---\\n\\n#### **II. What is LangChain?**  \\n- **Definition**: LangChain is an open-source framework designed to simplify the creation of applications powered by language models (e.g., GPT, LLaMA).  \\n- **Key Features**:  \\n  - **Modular Architecture**: Allows developers to connect language models, data sources, and outputs via \"chains.\"  \\n  - **Support for Multiple Models**: Integrates with various LLMs (e.g., OpenAI, Hugging Face).  \\n  - **Customization**: Enables building complex workflows (e.g., chatbots, data processing pipelines).  \\n- **Use Cases**:  \\n  - Building conversational AI agents.  \\n  - Creating data-driven applications that leverage language models.  \\n- **Notes**: Emphasize that LangChain is code-centric, requiring developers to write and structure their applications.\\n\\n---\\n\\n#### **III. What is LangSmith?**  \\n- **Definition**: LangSmith is a platform (often part of the LangChain ecosystem) that provides tools for building, testing, and deploying LangChain applications.  \\n- **Key Features**:  \\n  - **Visual Interface**: Allows developers to design and debug workflows using a drag-and-drop or configuration-based interface.  \\n  - **Collaboration Tools**: Facilitates team-based development and version control.  \\n  - **Integration with LangChain**: Acts as a \"bridge\" between code and deployment.  \\n- **Use Cases**:  \\n  - Rapid prototyping of LangChain applications.  \\n  - Debugging and optimizing complex language model workflows.  \\n- **Notes**: Highlight that LangSmith is not a replacement for LangChain but a tool to streamline development.\\n\\n---\\n\\n#### **IV. Key Differences Between LangChain and LangSmith**  \\n- **Function**:  \\n  - **LangChain**: Focuses on the *logic* and *structure* of applications.  \\n  - **LangSmith**: Focuses on the *development and deployment* of applications.  \\n- **User Base**:  \\n  - **LangChain**: Targeted at developers and engineers.  \\n  - **LangSmith**: Designed for teams requiring collaboration and visualization.  \\n- **Integration**:  \\n  - **LangChain** is the core framework.  \\n  - **LangSmith** enhances LangChain by providing tools for development, testing, and deployment.  \\n- **Example**:  \\n  - A developer uses **LangChain** to write code for a chatbot.  \\n  - They use **LangSmith** to test the chatbot’s performance and deploy it to production.\\n\\n---\\n\\n#### **V. Conclusion**  \\n- **Summary**: Recap the roles of LangChain (framework) and LangSmith (platform).  \\n- **Importance**: Emphasize how both tools work together to accelerate AI application development.  \\n- **Future Outlook**: Mention potential advancements in AI tooling and how LangChain and LangSmith might evolve.\\n\\n---\\n\\n### **Notes for Writing the Essay**  \\n1. **Clarity**: Avoid jargon. Explain terms like \"chains\" or \"LLMs\" for readers unfamiliar with AI.  \\n2. **Comparative Focus**: Use the \"Key Differences\" section to directly contrast the two tools.  \\n3. **Examples**: Include real-world scenarios (e.g., building a customer support chatbot) to illustrate their applications.  \\n4. **Tone**: Maintain a neutral, informative tone, avoiding bias toward one tool over the other.  \\n5. **Structure**: Ensure each section flows logically, with clear transitions.  \\n\\nLet me know if you’d like help expanding any specific section!'}}\n",
      "{'research_plan': {'content': ['LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', '# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison In the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like LangChain, LangGraph, LangFlow, and LangSmith. While these tools share some similarities, they cater to different aspects of AI application development. LangChain is an open-source framework designed to streamline the development of applications leveraging language models. LangFlow simplifies the prototyping process by providing a visual, drag-and-drop interface to design and test workflows. LangSmith is a monitoring and testing tool for LLM applications in production. By leveraging these tools effectively, you can streamline your AI development process and focus on delivering impactful solutions.', 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Compare LangChain, LangSmith, and Orq.ai to discover the best LLM development tools for building, deploying, and optimizing scalable AI applications. As a comprehensive platform for LLM product development, LangChain equips software teams with the tools needed to build, test, and deploy LLM-powered solutions at scale. While LangChain focuses on the flexibility and modularity required for building LLM-powered applications, LangSmith steps in to offer essential tools for deployment, monitoring, and optimization throughout the production process. Orq.ai offers several advantages over LangChain and LangSmith, providing a more integrated and efficient solution for LLM development. By offering an integrated solution that supports the entire LLM development lifecycle, Orq.ai enables teams to seamlessly build, deploy, and optimize LLM applications at scale, without needing to juggle multiple specialized tools.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the rapidly evolving field of artificial intelligence, frameworks and platforms play a pivotal role in streamlining the development of language model (LLM) applications. While LangChain serves as a foundational framework for constructing AI-powered tools, LangSmith functions as a collaborative platform that enhances the development lifecycle. Together, these tools cater to different aspects of AI application creation, offering developers a comprehensive ecosystem to innovate.  \\n\\nLangChain is an open-source framework designed to simplify the integration of language models into applications. Its modular architecture allows developers to create “chains” that connect LLMs, data sources, and outputs, enabling complex workflows like chatbots or data analysis pipelines. For instance, a developer might use LangChain to build a customer support chatbot by linking a GPT model to a database of product information. This code-centric approach prioritizes flexibility, making it ideal for engineers seeking granular control over their applications.  \\n\\nIn contrast, LangSmith is a platform that focuses on the development, testing, and deployment of these applications. While it integrates with LangChain, it provides a visual interface that allows teams to design workflows through drag-and-drop tools rather than writing code. This makes LangSmith particularly valuable for collaborative environments where non-developers or teams need to prototype and debug applications efficiently. For example, a product manager could use LangSmith to simulate a chatbot’s performance and refine its logic without needing to write a single line of code.  \\n\\nThe key distinction between the two lies in their primary functions: LangChain is a framework for building the logic of applications, while LangSmith is a platform for refining and deploying them. LangChain appeals to developers who prioritize customization, whereas LangSmith caters to teams requiring collaboration and visualization. Together, they form a symbiotic relationship—LangChain’s code-driven flexibility paired with LangSmith’s intuitive tools—enabling faster iteration and deployment of AI solutions.  \\n\\nIn conclusion, LangChain and LangSmith are not competitors but complementary tools in the AI development toolkit. While LangChain lays the groundwork for application logic, LangSmith elevates the process of testing and deployment. As AI continues to advance, the synergy between these platforms will likely deepen, empowering developers and teams to create more sophisticated and user-friendly applications. Their combined strengths underscore the importance of tailored tools in harnessing the full potential of language models.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Essay Critique and Recommendations**  \\n\\nYour essay provides a clear and concise overview of the differences between **LangChain** and **LangSmith**, effectively highlighting their complementary roles in AI development. The structure is logical, and the examples (e.g., chatbots, collaborative workflows) help clarify their purposes. However, there are opportunities to deepen the analysis, refine the depth of technical explanations, and enhance the essay’s overall impact. Below are detailed recommendations:  \\n\\n---\\n\\n### **Strengths**  \\n1. **Clear Organization**: The essay follows a logical flow, starting with an introduction, moving through detailed comparisons, and concluding with a synthesis of their roles.  \\n2. **Accessible Language**: The tone is professional yet approachable, making the content understandable for readers with varying levels of technical expertise.  \\n3. **Useful Examples**: The examples (e.g., customer support chatbots, product managers prototyping workflows) effectively illustrate the practical applications of both tools.  \\n\\n---\\n\\n### **Areas for Improvement**  \\n\\n#### **1. Depth of Technical Analysis**  \\nWhile the essay distinguishes between LangChain’s code-centric approach and LangSmith’s visual interface, it could benefit from **more detailed technical explanations**. For instance:  \\n- **LangChain**: Could elaborate on how \"chains\" are constructed (e.g., using Python libraries, specific functions, or integration with tools like LangChain’s `Chain` class).  \\n- **LangSmith**: Could clarify how its visual workflows map to underlying code (e.g., whether it generates code automatically or requires manual scripting).  \\n- **Integration**: A deeper discussion of how LangSmith *integrates* with LangChain (e.g., via APIs, shared data models, or deployment pipelines) would strengthen the essay.  \\n\\n**Recommendation**: Add 1–2 paragraphs explaining the technical architecture of each tool, including how they interact. For example:  \\n> \"LangChain’s chains are built using Python objects that define sequences of operations, such as `LLMChain` or `Pipeline`. In contrast, LangSmith’s visual interface translates these workflows into code via a backend engine, allowing users to test logic without writing code explicitly.\"  \\n\\n---\\n\\n#### **2. Comparative Depth**  \\nThe essay effectively contrasts the two tools but could **expand on their trade-offs**. For example:  \\n- **Flexibility vs. Usability**: While LangChain offers granular control, it may require more coding expertise. LangSmith’s visual tools lower the barrier to entry but might limit customization.  \\n- **Collaboration**: How does LangSmith’s collaborative environment (e.g., shared projects, version control) differ from LangChain’s individual or team workflows?  \\n- **Scalability**: Are there scenarios where one tool is better suited than the other (e.g., rapid prototyping vs. production deployment)?  \\n\\n**Recommendation**: Add a dedicated section (e.g., \"Trade-Offs and Use Cases\") to explore these dynamics. For example:  \\n> \"While LangChain’s code-centric approach is ideal for developers needing full control, LangSmith’s visual tools accelerate collaboration in cross-functional teams. For instance, a data scientist might use LangChain to fine-tune a model’s logic, while a product manager uses LangSmith to simulate user interactions.\"  \\n\\n---\\n\\n#### **3. Length and Scope**  \\nThe essay is concise but could be **expanded to 600–800 words** for greater depth. Consider:  \\n- **Real-World Applications**: Include case studies (e.g., a company using LangChain for a chatbot and LangSmith for A/B testing).  \\n- **Future Trends**: Discuss how these tools might evolve (e.g., AI-driven automation in LangSmith, or LangChain’s integration with emerging LLMs).  \\n- **Limitations**: Address potential drawbacks (e.g., LangSmith’s reliance on pre-built templates, or LangChain’s steep learning curve).  \\n\\n**Recommendation**: Add a paragraph on \"Broader Implications\" to contextualize their roles in AI’s future. For example:  \\n> \"As AI systems grow more complex, the synergy between tools like LangChain and LangSmith will become critical. LangChain’s flexibility ensures adaptability, while LangSmith’s collaboration features democratize AI development, enabling non-engineers to contribute meaningfully.\"  \\n\\n---\\n\\n#### **4. Stylistic and Structural Enhancements**  \\n- **Varied Sentence Structure**: Avoid repetitive phrasing (e.g., \"This makes LangSmith...\"). Use active voice and diverse sentence structures.  \\n- **Transitions**: Strengthen connections between paragraphs (e.g., \"Building on this, LangSmith’s visual interface...\").  \\n- **Tone**: Maintain a balance between technical precision and readability. For example, replace \"code-centric approach\" with \"programmable architecture\" for clarity.  \\n\\n**Recommendation**: Revise sentences like:  \\n> \"LangChain’s modular architecture allows developers to create \\'chains\\' that connect LLMs, data sources, and outputs, enabling complex workflows...\"  \\nto:  \\n> \"LangChain’s modular design empowers developers to construct \\'chains\\'—sequences of operations that link language models, databases, and outputs—enabling sophisticated workflows like chatbots or data pipelines.\"  \\n\\n---\\n\\n### **Final Thoughts**  \\nYour essay effectively communicates the core differences between LangChain and LangSmith. By expanding on technical details, exploring trade-offs, and adding real-world context, the essay could achieve greater depth and persuasiveness. These refinements would not only enhance the essay’s academic rigor but also better serve readers seeking to choose between these tools.  \\n\\n**Final Word Count**: ~500 words (expandable to 700–800 for deeper analysis).  \\n**Tone**: Professional, yet accessible.  \\n**Style**: Clear, logical, and example-driven.  \\n\\nLet me know if you’d like help rewriting or expanding specific sections!'}}\n",
      "{'research_critique': {'content': ['LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', '# LangChain vs LangGraph vs LangFlow vs LangSmith: A Detailed Comparison In the rapidly evolving world of AI, building applications powered by advanced language models like GPT-4 or Llama 3 has become more accessible, thanks to frameworks like LangChain, LangGraph, LangFlow, and LangSmith. While these tools share some similarities, they cater to different aspects of AI application development. LangChain is an open-source framework designed to streamline the development of applications leveraging language models. LangFlow simplifies the prototyping process by providing a visual, drag-and-drop interface to design and test workflows. LangSmith is a monitoring and testing tool for LLM applications in production. By leveraging these tools effectively, you can streamline your AI development process and focus on delivering impactful solutions.', 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Compare LangChain, LangSmith, and Orq.ai to discover the best LLM development tools for building, deploying, and optimizing scalable AI applications. As a comprehensive platform for LLM product development, LangChain equips software teams with the tools needed to build, test, and deploy LLM-powered solutions at scale. While LangChain focuses on the flexibility and modularity required for building LLM-powered applications, LangSmith steps in to offer essential tools for deployment, monitoring, and optimization throughout the production process. Orq.ai offers several advantages over LangChain and LangSmith, providing a more integrated and efficient solution for LLM development. By offering an integrated solution that supports the entire LLM development lifecycle, Orq.ai enables teams to seamlessly build, deploy, and optimize LLM applications at scale, without needing to juggle multiple specialized tools.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', \"🦜️🔗 LangChain 🦜️🔗 LangChain LangChain is a framework that consists of a number of packages. ## langchain-core\\u200b The main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture. `langchain` `langchain-openai`, `langchain-anthropic`, etc) so that they can be properly versioned and appropriately lightweight. `langchain-openai` `langchain-anthropic` ## langchain-community\\u200b This package contains third-party integrations that are maintained by the LangChain community. This contains integrations for various components (chat models, vector stores, tools, etc). `langgraph` is an extension of `langchain` aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. `langchain` A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\", 'With Datasets & Testing holding data and linked to agent runs. LangSmith is especially helpful when running autonomous agents, where the different steps or chains in the agent sequence is shown. In this article I only consider three of the five tools within LangSmith; Projects, Datasets & Testing & Hub. The code to run the agent: It is evident that the Agent is not performing optimal, hence LangSmith can be used to evaluate and improve the Agent. Automating metrics and AI-assisted feedback to evaluate agent performance is more time effective. The agent is run on each example, and evaluators are applied to the resulting run traces with automated generated feedback. Within LangSmith, the input, example reference output and the test results can be viewed.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\", 'Missing: Real- future trends', 'LangChain has emerged as a leading ecosystem providing three complementary tools: LangChain for building applications, LangGraph for creating stateful workflows, and LangSmith for monitoring and debugging. Built on top of LangChain, LangGraph uses a graph-based approach to define AI workflows as state machines. LangGraph shines in scenarios requiring complex decision-making workflows, multi-agent systems, iterative processes, or applications that need to handle various edge cases and error conditions. Examples include complex customer service workflows, multi-step data analysis pipelines, creative writing assistants that iterate on content, and research applications that need to gather information from multiple sources. When you run a LangChain or LangGraph application with LangSmith integration, every operation is automatically traced and logged. LangChain provides the foundational building blocks, LangGraph enables complex stateful workflows, and LangSmith ensures observability and continuous improvement.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the rapidly evolving field of artificial intelligence, frameworks and platforms play a pivotal role in streamlining the development of language model (LLM) applications. While LangChain serves as a foundational framework for constructing AI-powered tools, LangSmith functions as a collaborative platform that enhances the development lifecycle. Together, these tools cater to different aspects of AI application creation, offering developers a comprehensive ecosystem to innovate.  \\n\\nLangChain is an open-source framework designed to simplify the integration of language models into applications. Its modular architecture allows developers to create “chains” that connect LLMs, data sources, and outputs, enabling complex workflows like chatbots or data analysis pipelines. For instance, a developer might use LangChain to build a customer support chatbot by linking a GPT model to a database of product information. This code-centric approach prioritizes flexibility, making it ideal for engineers seeking granular control over their applications.  \\n\\nIn contrast, LangSmith is a platform that focuses on the development, testing, and deployment of these applications. While it integrates with LangChain, it provides a visual interface that allows teams to design workflows through drag-and-drop tools rather than writing code. This makes LangSmith particularly valuable for collaborative environments where non-developers or teams need to prototype and debug applications efficiently. For example, a product manager could use LangSmith to simulate a chatbot’s performance and refine its logic without needing to write a single line of code.  \\n\\nThe key distinction between the two lies in their primary functions: LangChain is a framework for building the logic of applications, while LangSmith is a platform for refining and deploying them. LangChain appeals to developers who prioritize customization, whereas LangSmith caters to teams requiring collaboration and visualization. Together, they form a symbiotic relationship—LangChain’s code-driven flexibility paired with LangSmith’s intuitive tools—enabling faster iteration and deployment of AI solutions.  \\n\\nIn conclusion, LangChain and LangSmith are not competitors but complementary tools in the AI development toolkit. While LangChain lays the groundwork for application logic, LangSmith elevates the process of testing and deployment. As AI continues to advance, the synergy between these platforms will likely deepen, empowering developers and teams to create more sophisticated and user-friendly applications. Their combined strengths underscore the importance of tailored tools in harnessing the full potential of language models.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, ValidationError\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.tools import tool\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from IPython.display import Image\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "# connect to tavily search tool - use your tavily api key\n",
    "os.environ['TAVILY_API_KEY']=\"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\"\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "#tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "#define agent state\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "#define and configure the model\n",
    "# configure model\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=10.0)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=chat_credentials[\"model_name\"], base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "\n",
    "#define prompts\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert writer tasked with writing a high level outline of an eassy. \\\n",
    "Write such an outline for the user provided topic. Give an outline of eassy along \\\n",
    "with any relevant notes or instructions for the sections.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an eassy assistant tasked with writing excellent 5-paragraph eassys. \\\n",
    "Generate the best eassy possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of ypur previous attempts. \\\n",
    "\n",
    "--------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a teacher grading an eassy submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when writing the following eassy. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as oulined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. \\\n",
    "Only generate 3 queries max.\n",
    "\"\"\"\n",
    "    \n",
    "def extract_json(text):\n",
    "    # Remove unwanted tags like <think> and <speak>\n",
    "    cleaned_text = re.sub(r'<\\/?[\\w\\d]+>', '', text).strip()\n",
    "\n",
    "    # Now try to extract the JSON part using regex\n",
    "    match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "def normalize_to_queries(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize LLM output into a dict matching the Queries schema.\n",
    "    Always returns: {\"queries\": [...]}.\n",
    "    Also logs the result as clean JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove <think>...</think> blocks if present\n",
    "    output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "    # Try strict JSON parse first\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        if isinstance(parsed, dict) and \"queries\" in parsed:\n",
    "            data = parsed\n",
    "        elif isinstance(parsed, list):\n",
    "            data = {\"queries\": parsed}\n",
    "        else:\n",
    "            raise ValueError(\"Invalid schema\")\n",
    "    except Exception:\n",
    "        # Fallback: treat as markdown/bullet/numbered list\n",
    "        lines = [\n",
    "            re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "            for line in output.splitlines() if line.strip()\n",
    "        ]\n",
    "        # Deduplicate while preserving order\n",
    "        seen = set()\n",
    "        unique_lines = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "        data = {\"queries\": unique_lines}\n",
    "\n",
    "    return data\n",
    "    \n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "#implement nodes\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\"plan\": response_content}\n",
    "    \n",
    "def research_plan_node(state: dict):\n",
    "    \"\"\"\n",
    "    Generates a research plan using a Qwen model and Tavily search.\n",
    "    Works without Pydantic.\n",
    "    \"\"\"\n",
    "    # Invoke Qwen model (plain text output)\n",
    "    raw_response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "            HumanMessage(content=state[\"task\"]),\n",
    "        ]\n",
    "    )\n",
    "    response_content = getattr(raw_response, \"content\", str(raw_response))\n",
    "\n",
    "    # Remove <think>...</think> if present\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Normalize into a list of queries (handle bullets, numbers, etc.)\n",
    "    lines = [\n",
    "        re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "        for line in response_content.splitlines()\n",
    "        if line.strip()\n",
    "    ]\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    queries_list = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "\n",
    "    # Initialize content\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    # Perform Tavily searches\n",
    "    for q in queries_list:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content_piece = r.get(\"content\", \"\")\n",
    "                if content_piece:\n",
    "                    content.append(str(content_piece))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "    }\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join([\"content\"] or [])\n",
    "    user_message = HumanMessage(content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\n",
    "        \"draft\": response_content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft']),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    return {\"critique\": response_content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    Generates a research plan using a Qwen model and Tavily search.\n",
    "    Works without Pydantic.\n",
    "    \"\"\"\n",
    "    # Invoke Qwen model (plain text output)\n",
    "    raw_response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state[\"critique\"]),\n",
    "        ]\n",
    "    )\n",
    "    response_content = getattr(raw_response, \"content\", str(raw_response))\n",
    "\n",
    "    # Remove <think>...</think> if present\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Normalize into a list of queries (handle bullets, numbers, etc.)\n",
    "    lines = [\n",
    "        re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "        for line in response_content.splitlines()\n",
    "        if line.strip()\n",
    "    ]\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    queries_list = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "\n",
    "    # Initialize content\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    # Perform Tavily searches\n",
    "    for q in queries_list:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content_piece = r.get(\"content\", \"\")\n",
    "                if content_piece:\n",
    "                    content.append(str(content_piece))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "    }\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "#build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "#set entry point\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "#define conditional edges\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "#define edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "#print graph\n",
    "#Image(graph.get_graph().draw_png())\n",
    "\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    for s in graph.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba91136-e012-4996-aa8e-409a3f70bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://465722abfb47c07dec.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://465722abfb47c07dec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import ewriter, writer_gui\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572a74a3-9c14-41a5-9131-80bd7064f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     +-----------+                  \n",
      "                     | __start__ |                  \n",
      "                     +-----------+                  \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                      +---------+                   \n",
      "                      | planner |                   \n",
      "                      +---------+                   \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                   +---------------+                \n",
      "                   | research_plan |                \n",
      "                   +---------------+                \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                      +----------+                  \n",
      "                      | generate |                  \n",
      "                   ...+----------+***               \n",
      "               ....         .        ****           \n",
      "           ....             .            ****       \n",
      "         ..                 .                ****   \n",
      "+---------+           +---------+                ** \n",
      "| __end__ |           | reflect |               **  \n",
      "+---------+           +---------+             **    \n",
      "                                ***         **      \n",
      "                                   *      **        \n",
      "                                    **   *          \n",
      "                            +-------------------+   \n",
      "                            | research_critique |   \n",
      "                            +-------------------+   \n"
     ]
    }
   ],
   "source": [
    "app.graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209d405-e9cf-427f-9ecc-99afc88c1815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
