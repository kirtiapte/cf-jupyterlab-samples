{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e741c4-bf73-4f95-8265-8e85ecf75e1f",
   "metadata": {},
   "source": [
    "## Langgraph Essay Writer Agent \n",
    "\n",
    "You are an expert writer tasked with writing a high level outline of an essay.  Write such an outline for user provided topic.  Give an outline of the essay along with notes and instructions for the sections\n",
    "\n",
    "##### Note: Go to JupyterLab terminal and execute following command before getting started\n",
    "<pre>\n",
    "    uv add langgraph\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316b0f92-e9cd-49d2-85fe-08a803e29513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': '### **Essay Outline: The Difference Between LangChain and LangSmith**\\n\\n---\\n\\n#### **I. Introduction**  \\n- **Purpose**: Introduce the topic by defining LangChain and LangSmith as tools in the AI/LLM ecosystem.  \\n- **Thesis Statement**: Highlight that while both are developed by the same team (or organization), they serve distinct purposes in AI application development.  \\n- **Notes**:  \\n  - Briefly mention the rise of LLM-powered tools and the need for frameworks like LangChain and platforms like LangSmith.  \\n  - Use a hook (e.g., \"In the rapidly evolving world of AI, LangChain and LangSmith have emerged as critical tools, but how do they differ?\").\\n\\n---\\n\\n#### **II. Overview of LangChain**  \\n- **Definition**: LangChain is a framework for building applications with large language models (LLMs).  \\n- **Key Features**:  \\n  - Modular architecture for integrating LLMs with external data sources.  \\n  - Tools for prompt engineering, memory management, and model orchestration.  \\n  - Focus on **application development** (e.g., chatbots, data pipelines).  \\n- **Target Audience**: Developers, data scientists, and AI engineers building LLM-powered applications.  \\n- **Notes**:  \\n  - Emphasize LangChain’s role in **data integration** and **workflow automation**.  \\n  - Mention its open-source nature and community-driven updates.  \\n\\n---\\n\\n#### **III. Overview of LangSmith**  \\n- **Definition**: LangSmith is a platform for **building, testing, and deploying AI applications**.  \\n- **Key Features**:  \\n  - Focus on **developer experience** (e.g., visual interfaces, debugging tools).  \\n  - Integration with LLMs and other AI models.  \\n  - Emphasis on **rapid prototyping** and **collaboration**.  \\n- **Target Audience**: Developers, product managers, and teams looking to **deploy AI solutions quickly**.  \\n- **Notes**:  \\n  - Highlight LangSmith’s role in **streamlining the development lifecycle** (e.g., testing, deployment).  \\n  - Compare it to tools like Streamlit or FastAPI for AI workflows.  \\n\\n---\\n\\n#### **IV. Key Differences Between LangChain and LangSmith**  \\n- **Purpose**:  \\n  - LangChain: **Framework for building LLM-powered applications**.  \\n  - LangSmith: **Platform for developing and deploying AI applications**.  \\n- **Features**:  \\n  - LangChain: Focus on **modular components** (e.g., prompts, data sources).  \\n  - LangSmith: Focus on **developer tools** (e.g., UIs, collaboration).  \\n- **User Base**:  \\n  - LangChain: For **technical users** building complex workflows.  \\n  - LangSmith: For **teams** needing to deploy AI solutions.  \\n- **Use Cases**:  \\n  - LangChain: Data pipelines, chatbots, and LLM-driven apps.  \\n  - LangSmith: Rapid prototyping, collaboration, and deployment.  \\n- **Notes**:  \\n  - Mention that LangSmith may be a newer tool or part of the LangChain ecosystem.  \\n  - Highlight that both tools complement each other in the AI development lifecycle.  \\n\\n---\\n\\n#### **V. Conclusion**  \\n- **Summary**: Recap the core differences (purpose, features, audience).  \\n- **Implications**: Discuss how choosing between LangChain and LangSmith depends on the project’s goals (e.g., complexity, deployment needs).  \\n- **Final Thought**: Emphasize the importance of understanding these tools in the context of AI development.  \\n- **Notes**:  \\n  - Suggest further research into their official documentation or case studies.  \\n  - Note that the tools may evolve, so readers should check for updates.  \\n\\n---\\n\\n### **Instructions for Expansion**  \\n1. **Research**: Verify details about LangChain and LangSmith (e.g., their official websites, GitHub repositories, or documentation).  \\n2. **Examples**: Include real-world use cases (e.g., a company using LangChain for data integration vs. LangSmith for deploying a chatbot).  \\n3. **Tone**: Maintain a balanced, informative tone, avoiding overly technical jargon.  \\n4. **Citations**: If using specific data or quotes, ensure proper attribution.  \\n\\nLet me know if you need help drafting the full essay!'}}\n",
      "entering research_plan_node\n",
      "json data in research plan mode: {'queries': ['\"LangChain vs LangSmith: Key differences\"', '\"What is the difference between LangChain and LangSmith?\"', '\"LangChain and LangSmith comparison: Features and use cases\"']}\n",
      "tavily search response: {'query': '\"LangChain vs LangSmith: Key differences\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Langchain vs Langsmith - Data Scientist's Diary\", 'url': 'https://medium.com/data-scientists-diary/langchain-vs-langsmith-1b525afd2098', 'content': 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'score': 0.9205687, 'raw_content': None}, {'title': 'Getting Started with LangChain for Beginners: Building RAG Made ...', 'url': 'https://ai.plainenglish.io/getting-started-with-langchain-for-beginners-building-rag-made-simple-6dbed2ba1be7', 'content': 'In this blog, we’ll walk through what LangChain is, how it helps build RAG systems, and what tools it gives you — like LangServe, LangSmith, and more. Task LangChain Component Load documents `DocumentLoader` classes Split text `CharacterTextSplitter`, `RecursiveCharacterTextSplitter` Generate embeddings OpenAI, HuggingFace, Ollama Store and search FAISS, Chroma, Pinecone Serve the app `LangServe` Monitor and debug `LangSmith` LangChain simplifies building end-to-end RAG systems — from loading your documents to deploying them as APIs. Whether you’re a hobbyist or an enterprise engineer, it gives you the building blocks to go from **text files to AI chatbots** in hours. ### Build intelligent RAG systems that know when to retrieve documents, search the web, or generate responses directly', 'score': 0.6307082, 'raw_content': None}], 'response_time': 1.07, 'request_id': '7fa95d19-fc89-4ea3-92d3-5044e00b080e'}\n",
      "tavily search response: {'query': '\"What is the difference between LangChain and LangSmith?\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.reddit.com/r/LangChain/comments/192avem/i_just_started_learning/', 'title': 'I just started learning. - LangChain - Reddit', 'content': 'What is the difference between LangChain and LangSmith? What do you think is the best way to learn this framework?', 'score': 0.98587, 'raw_content': None}, {'url': 'https://gist.github.com/ohmeow/f8ca7c10653717690337ca15b5d4b035', 'title': 'LangChain, LangSmith, LangServe Arxiv \"Research Assistant\"', 'content': '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', 'score': 0.98144, 'raw_content': None}], 'response_time': 0.82, 'request_id': 'efa1bc0e-f7e0-4495-a300-1cc8f4b270a9'}\n",
      "tavily search response: {'query': '\"LangChain and LangSmith comparison: Features and use cases\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'LangChain', 'url': 'https://www.langchain.com/', 'content': \"# The platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production. ###### Build agents with LangGraph Evaluate and observe agent performance at scale.###### DEPLOYMENT: STACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0Platform ### A full product suite for reliable agents and LLM apps Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain's frameworks or not. STACK 3:\\xa0Any agent framework +\\xa0LangGraph Platform LangGraph Platform works with *any* agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments. Get started with tools from the LangChain product suite for every step of the agent development lifecycle.\", 'score': 0.5030035, 'raw_content': None}, {'title': '探索 LangChain ：AI时代的编程革命！ - 文章 - 开发者社区 - 火山 …', 'url': 'https://developer.volcengine.com/articles/7540557608500133915', 'content': '1 day ago · 01 langchain简介📢 LangChain是一个开源的框架，专为大型语言模型（LLMs）驱动的应用开发而设计。 它通过提供模块化的构建块和组件，简化了整个应用生命周期，包括开发、生产化和部 …', 'score': 0.35267037, 'raw_content': None}], 'response_time': 1.65, 'request_id': 'f2877a80-d685-44de-a77e-7507a44f13b0'}\n",
      "exiting research_plan_node\n",
      "{'research_plan': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'In this blog, we’ll walk through what LangChain is, how it helps build RAG systems, and what tools it gives you — like LangServe, LangSmith, and more. Task LangChain Component Load documents `DocumentLoader` classes Split text `CharacterTextSplitter`, `RecursiveCharacterTextSplitter` Generate embeddings OpenAI, HuggingFace, Ollama Store and search FAISS, Chroma, Pinecone Serve the app `LangServe` Monitor and debug `LangSmith` LangChain simplifies building end-to-end RAG systems — from loading your documents to deploying them as APIs. Whether you’re a hobbyist or an enterprise engineer, it gives you the building blocks to go from **text files to AI chatbots** in hours. ### Build intelligent RAG systems that know when to retrieve documents, search the web, or generate responses directly', 'What is the difference between LangChain and LangSmith? What do you think is the best way to learn this framework?', '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', \"# The platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production. ###### Build agents with LangGraph Evaluate and observe agent performance at scale.###### DEPLOYMENT: STACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0Platform ### A full product suite for reliable agents and LLM apps Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain's frameworks or not. STACK 3:\\xa0Any agent framework +\\xa0LangGraph Platform LangGraph Platform works with *any* agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments. Get started with tools from the LangChain product suite for every step of the agent development lifecycle.\", '1 day ago · 01 langchain简介📢 LangChain是一个开源的框架，专为大型语言模型（LLMs）驱动的应用开发而设计。 它通过提供模块化的构建块和组件，简化了整个应用生命周期，包括开发、生产化和部 …']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the rapidly evolving world of AI, LangChain and LangSmith have emerged as critical tools for developers, yet their roles remain distinct. While both are developed by the same team, they cater to different stages of AI application development. LangChain functions as a framework for building LLM-powered tools, whereas LangSmith serves as a platform for testing and deploying these applications. Understanding their differences is essential for leveraging their strengths effectively.  \\n\\nLangChain is designed to streamline the creation of AI applications by offering modular components that integrate large language models (LLMs) with external data sources. Its features include tools for prompt engineering, memory management, and model orchestration, making it ideal for complex workflows like data pipelines or chatbots. Developers and engineers often use LangChain to construct applications that require seamless interaction between LLMs and structured data. Its open-source nature and community-driven updates further enhance its flexibility.  \\n\\nIn contrast, LangSmith focuses on the development and deployment lifecycle, providing a user-friendly platform for testing, debugging, and collaborating on AI projects. It emphasizes rapid prototyping through visual interfaces and collaborative tools, appealing to teams aiming to deploy solutions quickly. While LangChain is technical and workflow-oriented, LangSmith prioritizes developer experience, making it accessible to non-experts and fostering cross-functional collaboration.  \\n\\nThe key differences between the two lie in their purpose, features, and target audiences. LangChain excels in building intricate applications with LLMs, catering to technical users who need control over data integration and automation. LangSmith, on the other hand, simplifies the deployment process, appealing to teams focused on collaboration and speed. For instance, a data scientist might use LangChain to design a custom chatbot, while a product team might rely on LangSmith to refine and launch the same application. Together, they form a cohesive ecosystem for AI development.  \\n\\nIn conclusion, LangChain and LangSmith serve complementary roles in the AI development lifecycle. While LangChain is a framework for constructing LLM-powered applications, LangSmith is a platform for testing and deploying them. The choice between the two depends on the project’s complexity and goals—whether building from scratch or accelerating deployment. As AI tools continue to evolve, understanding their distinctions will empower developers to harness their full potential.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Essay Critique and Recommendations**  \\n\\nYour essay provides a clear and structured comparison of **LangChain** and **LangSmith**, effectively highlighting their distinct roles in AI development. The writing is concise, and the examples (e.g., a data scientist using LangChain vs. a product team using LangSmith) help clarify their use cases. However, there are opportunities to deepen the analysis, expand on technical details, and refine the tone for greater clarity and engagement. Below are specific recommendations:  \\n\\n---\\n\\n### **Strengths of the Essay**  \\n1. **Clear Organization**: The essay follows a logical flow, starting with an introduction, then defining each tool, comparing their features, and concluding with their complementary roles.  \\n2. **Accessible Language**: The explanation of technical concepts (e.g., \"prompt engineering,\" \"model orchestration\") is straightforward and avoids excessive jargon.  \\n3. **Useful Examples**: The hypothetical scenarios (e.g., a chatbot built with LangChain) help readers visualize real-world applications.  \\n\\n---\\n\\n### **Areas for Improvement**  \\n\\n#### **1. Depth of Technical Details**  \\nWhile the essay mentions features like \"modular components\" and \"visual interfaces,\" it could benefit from **more specific technical explanations**. For example:  \\n- **LangChain**: What exactly does \"model orchestration\" entail? How does it differ from traditional software development workflows?  \\n- **LangSmith**: What does \"rapid prototyping through visual interfaces\" look like? Are there APIs or integrations with tools like Docker or Kubernetes?  \\n- **Open-Source vs. Platform**: How do their licensing models or community-driven updates impact their use cases?  \\n\\n**Recommendation**: Add **technical depth** by:  \\n- Explaining key features (e.g., \"LangChain’s `Chain` class allows developers to compose multiple LLMs into a single pipeline\").  \\n- Including **code snippets or pseudocode** (e.g., \"A simple LangChain workflow might involve: `from langchain import Chain`\").  \\n- Discussing **trade-offs** (e.g., \"While LangChain offers flexibility, its complexity may require advanced Python skills\").  \\n\\n---\\n\\n#### **2. Length and Scope**  \\nThe essay is concise but could benefit from **expanding to 500–600 words** to:  \\n- **Elaborate on use cases**: For instance, how does LangSmith’s \"collaborative tools\" differ from traditional project management software? What features (e.g., version control, real-time editing) make it unique?  \\n- **Compare with alternatives**: Are there other tools (e.g., Hugging Face, AutoML) that serve similar purposes? How do LangChain and LangSmith differentiate from them?  \\n- **Address limitations**: Are there scenarios where one tool might be less effective? For example, does LangSmith’s focus on speed compromise customization?  \\n\\n**Recommendation**:  \\n- **Add a section** on **real-world applications** (e.g., \"A healthcare startup might use LangChain to build a diagnostic chatbot, while LangSmith helps them deploy it across multiple platforms\").  \\n- **Include a **\"Limitations\"** paragraph** to balance the analysis.  \\n\\n---\\n\\n#### **3. Style and Engagement**  \\nThe essay is informative but could be more **engaging** by:  \\n- **Using rhetorical questions**: \"What if a developer needs to test a complex AI workflow without writing code?\"  \\n- **Analogies**: \"LangChain is like a construction kit for AI apps, while LangSmith is the final inspection before launch.\"  \\n- **Tone**: Avoid overly formal phrasing (e.g., \"The key differences between the two lie in their purpose...\"). Replace with more dynamic language: \"At their core, LangChain and LangSmith serve different stages of AI development—like a blueprint versus a construction site.\"  \\n\\n**Recommendation**:  \\n- **Revise the opening** to hook readers: \"In the AI development lifecycle, two tools stand out: LangChain and LangSmith. But what exactly sets them apart, and why does it matter?\"  \\n- **Use active voice** (e.g., \"LangChain enables developers to build...\" instead of \"Developers are enabled by LangChain...\").  \\n\\n---\\n\\n#### **4. Clarity and Precision**  \\nSome phrases could be **more precise**:  \\n- \"LangSmith serves as a platform for testing and deploying these applications\" → \"LangSmith streamlines the testing, debugging, and deployment of AI applications.\"  \\n- \"Catering to technical users\" → \"Targeting developers and engineers with expertise in Python and AI workflows.\"  \\n\\n**Recommendation**:  \\n- **Avoid vague terms** like \"complex workflows\" or \"speed.\" Instead, specify: \"LangSmith’s drag-and-drop interface reduces deployment time by 40% compared to traditional methods.\"  \\n- **Define acronyms** (e.g., \"LLMs\" in the first paragraph).  \\n\\n---\\n\\n### **Final Recommendations**  \\n1. **Expand depth**: Add technical details, code examples, and real-world use cases.  \\n2. **Increase length**: Aim for 500–600 words to allow for nuanced analysis.  \\n3. **Enhance engagement**: Use analogies, rhetorical questions, and active voice.  \\n4. **Refine clarity**: Replace vague terms with precise language and define acronyms.  \\n\\nBy addressing these areas, your essay will become a more comprehensive and compelling resource for readers seeking to understand the roles of LangChain and LangSmith in AI development.'}}\n",
      "Search failed for query 'Technical deep dive into LangChain's model orchestration and LangSmith's visual interface for AI workflows': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query 'Real-world use cases comparing LangChain and LangSmith in healthcare, enterprise, and startup applications': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query 'Comparative analysis of LangChain and LangSmith with alternatives like Hugging Face, AutoML, and Docker/Kubernetes integrations': can only concatenate str (not \"dict\") to str\n",
      "{'research_critique': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'In this blog, we’ll walk through what LangChain is, how it helps build RAG systems, and what tools it gives you — like LangServe, LangSmith, and more. Task LangChain Component Load documents `DocumentLoader` classes Split text `CharacterTextSplitter`, `RecursiveCharacterTextSplitter` Generate embeddings OpenAI, HuggingFace, Ollama Store and search FAISS, Chroma, Pinecone Serve the app `LangServe` Monitor and debug `LangSmith` LangChain simplifies building end-to-end RAG systems — from loading your documents to deploying them as APIs. Whether you’re a hobbyist or an enterprise engineer, it gives you the building blocks to go from **text files to AI chatbots** in hours. ### Build intelligent RAG systems that know when to retrieve documents, search the web, or generate responses directly', 'What is the difference between LangChain and LangSmith? What do you think is the best way to learn this framework?', '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', \"# The platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production. ###### Build agents with LangGraph Evaluate and observe agent performance at scale.###### DEPLOYMENT: STACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0Platform ### A full product suite for reliable agents and LLM apps Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain's frameworks or not. STACK 3:\\xa0Any agent framework +\\xa0LangGraph Platform LangGraph Platform works with *any* agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments. Get started with tools from the LangChain product suite for every step of the agent development lifecycle.\", '1 day ago · 01 langchain简介📢 LangChain是一个开源的框架，专为大型语言模型（LLMs）驱动的应用开发而设计。 它通过提供模块化的构建块和组件，简化了整个应用生命周期，包括开发、生产化和部 …']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the rapidly evolving world of AI, LangChain and LangSmith have emerged as critical tools for developers, yet their roles remain distinct. While both are developed by the same team, they cater to different stages of AI application development. LangChain functions as a framework for building LLM-powered tools, whereas LangSmith serves as a platform for testing and deploying these applications. Understanding their differences is essential for leveraging their strengths effectively.  \\n\\nLangChain is designed to streamline the creation of AI applications by offering modular components that integrate large language models (LLMs) with external data sources. Its features include tools for prompt engineering, memory management, and model orchestration, making it ideal for complex workflows like data pipelines or chatbots. Developers and engineers often use LangChain to construct applications that require seamless interaction between LLMs and structured data. Its open-source nature and community-driven updates further enhance its flexibility.  \\n\\nIn contrast, LangSmith focuses on the development and deployment lifecycle, providing a user-friendly platform for testing, debugging, and collaborating on AI projects. It emphasizes rapid prototyping through visual interfaces and collaborative tools, appealing to teams aiming to deploy solutions quickly. While LangChain is technical and workflow-oriented, LangSmith prioritizes developer experience, making it accessible to non-experts and fostering cross-functional collaboration.  \\n\\nThe key differences between the two lie in their purpose, features, and target audiences. LangChain excels in building intricate applications with LLMs, catering to technical users who need control over data integration and automation. LangSmith, on the other hand, simplifies the deployment process, appealing to teams focused on collaboration and speed. For instance, a data scientist might use LangChain to design a custom chatbot, while a product team might rely on LangSmith to refine and launch the same application. Together, they form a cohesive ecosystem for AI development.  \\n\\nIn conclusion, LangChain and LangSmith serve complementary roles in the AI development lifecycle. While LangChain is a framework for constructing LLM-powered applications, LangSmith is a platform for testing and deploying them. The choice between the two depends on the project’s complexity and goals—whether building from scratch or accelerating deployment. As AI tools continue to evolve, understanding their distinctions will empower developers to harness their full potential.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, ValidationError\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.tools import tool\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from IPython.display import Image\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# connect to tavily search tool - use your tavily api key\n",
    "os.environ['TAVILY_API_KEY']=\"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\"\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "#tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "#define agent state\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "#define and configure the model\n",
    "# configure model\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=10.0)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=chat_credentials[\"model_name\"], base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "\n",
    "#define prompts\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert writer tasked with writing a high level outline of an eassy. \\\n",
    "Write such an outline for the user provided topic. Give an outline of eassy along \\\n",
    "with any relevant notes or instructions for the sections.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an eassy assistant tasked with writing excellent 5-paragraph eassys. \\\n",
    "Generate the best eassy possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of ypur previous attempts. \\\n",
    "\n",
    "--------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a teacher grading an eassy submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when writing the following eassy. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as oulined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. \\\n",
    "Only generate 3 queries max.\n",
    "\"\"\"\n",
    "    \n",
    "def extract_json(text):\n",
    "    # Remove unwanted tags like <think> and <speak>\n",
    "    cleaned_text = re.sub(r'<\\/?[\\w\\d]+>', '', text).strip()\n",
    "\n",
    "    # Now try to extract the JSON part using regex\n",
    "    match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "def normalize_to_queries(output: str):\n",
    "    \"\"\"\n",
    "    Convert model output into a dict matching Queries schema.\n",
    "    \"\"\"\n",
    "    # Remove <think>...</think> if present\n",
    "    output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Try strict JSON parse\n",
    "    try:\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: convert markdown/bullets/numbered list into dict\n",
    "        lines = [\n",
    "            re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"')\n",
    "            for line in output.splitlines()\n",
    "            if line.strip()\n",
    "        ]\n",
    "        return {\"queries\": lines}\n",
    "    \n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "#implement nodes\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\"plan\": response_content}\n",
    "    \n",
    "def research_plan_node(state: AgentState):\n",
    "    print(\"entering research_plan_node\")\n",
    "    raw_response = model.invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    response_content = raw_response.content if hasattr(raw_response, 'content') else str(raw_response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    try:\n",
    "        # Try parsing as JSON first\n",
    "        try:\n",
    "            json_data = json.loads(response_content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: convert numbered/bulleted list into dict\n",
    "            lines = [line.strip(\"0123456789. -\") for line in response_content.splitlines() if line.strip()]\n",
    "            json_data = {\"queries\": lines}\n",
    "\n",
    "        print(\"json data in research plan mode:\", json_data)\n",
    "        response_content = normalize_to_queries(response_content)\n",
    "        queries = Queries.model_validate(json_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from model output:\", e)\n",
    "        return {\"content\": state.get('content', [])}\n",
    "\n",
    "    content = state.get('content', [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        print(\"tavily search response:\", response)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "\n",
    "    print(\"exiting research_plan_node\")\n",
    "    return {\"content\": content}\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join([\"content\"] or [])\n",
    "    user_message = HumanMessage(content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\n",
    "        \"draft\": response_content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft']),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    return {\"critique\": response_content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    try:\n",
    "        # Run the model\n",
    "        raw_response = model.invoke([\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state['critique'])\n",
    "        ])\n",
    "        # Safely get the content\n",
    "        response_content = raw_response.content if hasattr(raw_response, 'content') else str(raw_response)\n",
    "        \n",
    "\n",
    "        # Remove <think>...</think> blocks completely\n",
    "        response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL)\n",
    "\n",
    "        # Extract JSON from model output (if Qwen adds extra tokens)\n",
    "        response_content = normalize_to_queries(response_content)\n",
    "        queries = Queries.model_validate(response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during model invocation or JSON parsing:\", e)\n",
    "        return {\"content\": state.get(\"content\", [])}\n",
    "\n",
    "    content = state.get(\"content\", [])\n",
    "    for q in queries.queries:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            print(\"tavily response\" + response)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content.append(r.get(\"content\", \"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "    \n",
    "    return {\"content\": content}\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "#build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "#set entry point\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "#define conditional edges\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "#define edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "#print graph\n",
    "#Image(graph.get_graph().draw_png())\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    for s in graph.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba91136-e012-4996-aa8e-409a3f70bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://a3f1b03e76dc2eec3b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a3f1b03e76dc2eec3b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import ewriter, writer_gui\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a74a3-9c14-41a5-9131-80bd7064f444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
