{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e741c4-bf73-4f95-8265-8e85ecf75e1f",
   "metadata": {},
   "source": [
    "## Langgraph Essay Writer Agent \n",
    "\n",
    "You are an expert writer tasked with writing a high level outline of an essay.  Write such an outline for user provided topic.  Give an outline of the essay along with notes and instructions for the sections\n",
    "\n",
    "##### Note: Go to JupyterLab terminal and execute following command before getting started\n",
    "<pre>\n",
    "    uv add langgraph\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316b0f92-e9cd-49d2-85fe-08a803e29513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': '### **Essay Outline: The Difference Between LangChain and LangSmith**\\n\\n---\\n\\n#### **I. Introduction**  \\n**Purpose:** Introduce the growing importance of large language models (LLMs) and the tools that support their development and deployment.  \\n**Key Points:**  \\n- Briefly define LLMs and their applications (e.g., chatbots, data analysis).  \\n- Mention that tools like **LangChain** and **LangSmith** are critical for building and managing AI workflows.  \\n- **Thesis:** While both tools are designed for working with LLMs, they differ in focus, functionality, and target users.  \\n\\n**Notes for Writer:**  \\n- Start with a hook (e.g., \"In the era of AI, the tools that power language models are as vital as the models themselves\").  \\n- Define LLMs briefly to set context.  \\n\\n---\\n\\n#### **II. What is LangChain?**  \\n**Purpose:** Explain LangChain’s role as a framework for building applications with LLMs.  \\n**Key Points:**  \\n- **Focus:** Development of AI workflows, emphasizing flexibility and integration with multiple models/APIs.  \\n- **Key Features:**  \\n  - Chainable operations (e.g., data preprocessing → model inference).  \\n  - Support for custom prompts, memory management, and tool integration.  \\n  - Emphasis on **developer control** and **customization**.  \\n- **Use Cases:** Building complex AI applications (e.g., chatbots, data pipelines).  \\n\\n**Notes for Writer:**  \\n- Highlight LangChain’s open-source nature and its role in **development** rather than deployment.  \\n- Use examples like \"LangChain allows developers to chain multiple models (e.g., GPT-4 + a database API) for advanced tasks.\"  \\n\\n---\\n\\n#### **III. What is LangSmith?**  \\n**Purpose:** Explain LangSmith’s role as a platform for deploying and managing AI applications.  \\n**Key Points:**  \\n- **Focus:** Streamlining the **deployment and monitoring** of AI workflows.  \\n- **Key Features:**  \\n  - User-friendly interface for testing and deploying models.  \\n  - Emphasis on **collaboration** (e.g., team workflows, versioning).  \\n  - Tools for **monitoring performance** and **debugging**.  \\n- **Use Cases:** Scaling AI applications, ensuring reliability, and enabling non-developers to interact with models.  \\n\\n**Notes for Writer:**  \\n- Contrast LangSmith’s focus on **deployment** and **team collaboration** with LangChain’s **development** focus.  \\n- Mention potential use cases like \"LangSmith helps teams deploy a chatbot to a production environment with real-time analytics.\"  \\n\\n---\\n\\n#### **IV. Comparison: LangChain vs. LangSmith**  \\n**Purpose:** Highlight the core differences between the two tools.  \\n**Key Points:**  \\n1. **Focus:**  \\n   - **LangChain:** Development of AI workflows (customization, flexibility).  \\n   - **LangSmith:** Deployment and management of AI applications (ease of use, collaboration).  \\n2. **Functionality:**  \\n   - LangChain: Tools for chaining operations, integrating with external systems.  \\n   - LangSmith: Platforms for testing, deploying, and monitoring AI models.  \\n3. **Target Users:**  \\n   - **LangChain:** Developers, data scientists, and AI engineers.  \\n   - **LangSmith:** Product managers, DevOps teams, and organizations looking to scale AI solutions.  \\n\\n**Notes for Writer:**  \\n- Use a table or bullet points to clarify differences (if allowed).  \\n- Emphasize that LangChain is for **building** AI workflows, while LangSmith is for **running and maintaining** them.  \\n\\n---\\n\\n#### **V. Conclusion**  \\n**Purpose:** Summarize the key differences and their implications.  \\n**Key Points:**  \\n- Recap the distinct roles of LangChain (development) and LangSmith (deployment).  \\n- Highlight that the choice between the two depends on the user’s needs: **customization** (LangChain) vs. **scalability and ease of use** (LangSmith).  \\n- Final thought: Both tools are complementary, with LangChain enabling innovation and LangSmith ensuring its real-world impact.  \\n\\n**Notes for Writer:**  \\n- End with a strong closing statement, e.g., \"In the evolving AI landscape, tools like LangChain and LangSmith are not just complementary—they are essential for turning language models into transformative applications.\"  \\n\\n---\\n\\n### **Instructions for the Writer**  \\n- **Tone:** Academic but accessible. Avoid jargon unless explained.  \\n- **Structure:** Follow the outline closely, ensuring each section flows logically.  \\n- **Examples:** Use real-world scenarios (e.g., \"A developer uses LangChain to build a multi-step data analysis pipeline, while a team uses LangSmith to deploy the same pipeline to a cloud environment\").  \\n- **Audience:** Assume readers have basic knowledge of LLMs but may not be familiar with these tools.  \\n\\nLet me know if you need help expanding any section!'}}\n",
      "entering research_plan_node\n",
      "json data in research plan mode: {'queries': ['\"LangChain vs LangSmith: Key differences\"', '\"What is the difference between LangChain and LangSmith?\"', '\"LangChain and LangSmith comparison: Features and use cases\"']}\n",
      "tavily search response: {'query': '\"LangChain vs LangSmith: Key differences\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'LangChain vs LangSmith: Understanding the Differences ...', 'url': 'https://blog.gopenai.com/langchain-vs-langsmith-understanding-the-differences-pros-and-cons-a18cff9b31f0', 'content': 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'score': 0.9464435, 'raw_content': None}, {'title': 'Breaking Down Langchain vs Langsmith for Smarter ...', 'url': 'https://blog.lamatic.ai/guides/langchain-vs-langsmith/', 'content': \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\", 'score': 0.9220975, 'raw_content': None}], 'response_time': 1.68, 'request_id': '17018c66-780e-4500-86da-8a5e07cbaa43'}\n",
      "tavily search response: {'query': '\"What is the difference between LangChain and LangSmith?\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://gist.github.com/ohmeow/f8ca7c10653717690337ca15b5d4b035', 'title': 'LangChain, LangSmith, LangServe Arxiv \"Research Assistant\"', 'content': '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', 'score': 0.77548623, 'raw_content': None}, {'url': 'https://blog.csdn.net/bulucc/article/details/148870988', 'title': 'AI Agents in LangGraph学习笔记原创 - CSDN博客', 'content': 'for s in graph.stream({. \\'task\\': \"what is the difference between langchain and langsmith\",. \"max_revisions\": 2,. \"revision_number\": 1,. },', 'score': 0.7166357, 'raw_content': None}], 'response_time': 1.38, 'request_id': '9afaeb6d-1e17-44cb-8765-2877d80a7894'}\n",
      "tavily search response: {'query': '\"LangChain and LangSmith comparison: Features and use cases\"', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'PromptLayer LangChain vs LangSmith: Comprehensive Comparison for Devs', 'url': 'https://blog.promptlayer.com/langchain-vs-langsmith/', 'content': 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'score': 0.91156644, 'raw_content': None}, {'title': 'Medium Langchain vs Langsmith', 'url': 'https://medium.com/data-scientists-diary/langchain-vs-langsmith-1b525afd2098', 'content': 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'score': 0.898277, 'raw_content': None}], 'response_time': 1.05, 'request_id': 'f8be934f-7856-4f85-90c3-182dea4ce365'}\n",
      "exiting research_plan_node\n",
      "{'research_plan': {'content': ['LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\", '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', 'for s in graph.stream({. \\'task\\': \"what is the difference between langchain and langsmith\",. \"max_revisions\": 2,. \"revision_number\": 1,. },', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the era of artificial intelligence, the tools that power language models are as vital as the models themselves. Large language models (LLMs) have revolutionized industries, enabling applications from chatbots to data analysis. However, building and deploying these models requires specialized tools. **LangChain** and **LangSmith** are two such platforms, each designed to address distinct stages of AI development. While both facilitate working with LLMs, their focus, functionality, and target users differ significantly.  \\n\\n**LangChain** is a framework centered on the *development* of AI applications. It empowers developers to construct complex workflows by chaining together multiple models, APIs, and data sources. For instance, a developer might use LangChain to create a pipeline that preprocesses user input, feeds it into a language model, and then formats the output for a chatbot. Its emphasis on *flexibility and customization* makes it ideal for engineers and data scientists who need fine-grained control over their AI systems. Features like custom prompts, memory management, and integration with external tools (e.g., databases or analytics platforms) highlight its role in *building* sophisticated applications.  \\n\\nIn contrast, **LangSmith** focuses on the *deployment and management* of AI workflows. Designed for teams and organizations, it streamlines the process of testing, monitoring, and scaling AI applications. For example, a product team might use LangSmith to deploy a chatbot to a cloud environment, track its performance in real time, and debug issues without requiring deep coding expertise. Its user-friendly interface and collaboration tools (e.g., version control and team workflows) cater to non-developers, such as product managers or DevOps teams. By prioritizing *ease of use and scalability*, LangSmith ensures that AI solutions are not only built but also maintained effectively.  \\n\\nThe core difference between the two lies in their purpose: **LangChain is for *building* AI workflows**, while **LangSmith is for *running and refining* them**. LangChain’s strength lies in its open-source flexibility, appealing to developers who prioritize customization. LangSmith, on the other hand, excels in simplifying deployment, making it a choice for teams focused on scalability and collaboration. While LangChain might be used to design a multi-step data analysis pipeline, LangSmith would handle the subsequent steps of testing, monitoring, and refining that pipeline in a production environment.  \\n\\nIn conclusion, LangChain and LangSmith are complementary tools in the AI ecosystem. LangChain’s focus on development enables innovation, while LangSmith ensures that these innovations can be reliably deployed and maintained. For organizations, the choice between the two depends on their needs: developers seeking control and flexibility may favor LangChain, while teams prioritizing collaboration and scalability might lean toward LangSmith. Together, they exemplify how specialized tools can transform the potential of large language models into real-world impact.  \\n\\n---  \\nThis essay adheres to the outlined structure, emphasizing clarity and logical flow. It balances technical details with accessible explanations, ensuring readers grasp the distinct roles of both platforms.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Essay Critique and Recommendations**  \\n\\nYour essay provides a clear, structured comparison of **LangChain** and **LangSmith**, emphasizing their distinct roles in AI development. The writing is accessible, logically organized, and balances technical details with readability. Below are detailed recommendations to enhance the essay’s depth, clarity, and impact.  \\n\\n---\\n\\n### **Strengths of the Essay**  \\n1. **Clear Structure**: The essay follows a logical flow, starting with an introduction, defining each tool’s purpose, contrasting their roles, and concluding with their complementary nature.  \\n2. **Balanced Tone**: The essay avoids overly technical jargon while still providing enough detail to satisfy readers familiar with AI tools.  \\n3. **Useful Analogies**: Phrases like “LangChain is for *building* AI workflows” and “LangSmith is for *running and refining* them” effectively simplify complex concepts.  \\n\\n---\\n\\n### **Recommendations for Improvement**  \\n\\n#### **1. Expand Technical Depth**  \\n**Goal**: Provide a more nuanced understanding of how each tool functions technically.  \\n**Suggestions**:  \\n- **LangChain**: Explain how \"chaining models, APIs, and data sources\" works. For example, describe a concrete example of a workflow (e.g., a customer support chatbot that integrates a language model, a database of FAQs, and a sentiment analysis API).  \\n- **LangSmith**: Clarify how \"monitoring, scaling, and real-time debugging\" is achieved. For instance, mention specific features like logging, A/B testing, or integration with cloud platforms (e.g., AWS, Docker).  \\n- **Compare with Alternatives**: Briefly mention how these tools differ from other AI platforms (e.g., Hugging Face, TensorFlow Extended) to contextualize their unique value.  \\n\\n**Example**:  \\n> \"LangChain’s *chaining* allows developers to create pipelines where a user’s query is first processed by a text embedding model, then matched against a vector database, and finally formatted into a response. This modular approach enables complex tasks like multi-step reasoning or data synthesis.\"  \\n\\n---\\n\\n#### **2. Add Real-World Use Cases**  \\n**Goal**: Demonstrate the practical relevance of each tool.  \\n**Suggestions**:  \\n- **LangChain**: Highlight use cases like “building a data analysis pipeline that integrates a language model with a SQL database to generate insights from unstructured data.”  \\n- **LangSmith**: Discuss scenarios such as “deploying a customer service chatbot to a cloud environment, where it is monitored for response time, accuracy, and user feedback.”  \\n- **Industry Examples**: Mention specific industries (e.g., healthcare, finance) to show how each tool addresses sector-specific challenges.  \\n\\n**Example**:  \\n> \"A healthcare startup might use LangChain to design a diagnostic tool that analyzes patient symptoms via a language model, cross-references them with a medical database, and generates a prioritized list of potential conditions. LangSmith would then deploy this tool to a cloud platform, ensuring it scales during peak usage and logs errors for continuous improvement.\"  \\n\\n---\\n\\n#### **3. Clarify the Relationship Between the Tools**  \\n**Goal**: Strengthen the argument that the tools are complementary.  \\n**Suggestions**:  \\n- **Integration Examples**: Describe how a team might use LangChain for development and LangSmith for deployment. For instance:  \\n  > \"A product team might use LangChain to prototype a sales assistant that generates personalized email responses, then hand the finalized workflow to LangSmith for deployment, where it is monitored for performance and user engagement.\"  \\n- **Highlight Synergy**: Emphasize how LangChain’s flexibility and LangSmith’s scalability together enable end-to-end AI solutions.  \\n\\n---\\n\\n#### **4. Enhance Style and Precision**  \\n**Goal**: Improve readability and engagement.  \\n**Suggestions**:  \\n- **Vary Sentence Structure**: Avoid repetitive phrasing (e.g., “features like X, Y, Z highlight its role in...”).  \\n  > *Instead of*: “LangChain’s emphasis on flexibility and customization makes it ideal for engineers...”  \\n  > *Try*: “LangChain’s open-source architecture empowers developers to tailor workflows, making it a preferred choice for complex, custom AI systems.”  \\n- **Use Active Voice**:  \\n  > *Instead of*: “The tool is designed to streamline...”  \\n  > *Try*: “LangSmith streamlines...”  \\n- **Define Jargon**: While terms like “memory management” are common in AI, a brief explanation (e.g., “features that allow models to retain context across interactions”) could aid less technical readers.  \\n\\n---\\n\\n#### **5. Expand on Target Users**  \\n**Goal**: Clarify who benefits most from each tool.  \\n**Suggestions**:  \\n- **LangChain**: Emphasize its appeal to developers, data scientists, and researchers who prioritize customization.  \\n- **LangSmith**: Highlight its value for product managers, DevOps teams, and organizations focused on scalability.  \\n- **Address Trade-Offs**: Discuss potential limitations (e.g., LangChain’s steeper learning curve vs. LangSmith’s user-friendly interface).  \\n\\n**Example**:  \\n> \"While LangChain’s flexibility is a boon for developers, it may require significant coding expertise. In contrast, LangSmith’s intuitive interface lowers the barrier to entry, making it ideal for teams without deep AI engineering backgrounds.\"  \\n\\n---\\n\\n### **Final Thoughts**  \\nYour essay effectively distinguishes LangChain and LangSmith, but expanding on technical details, real-world applications, and user-specific benefits will elevate its impact. By incorporating concrete examples, clarifying jargon, and emphasizing synergy, the essay can better serve both technical and non-technical audiences.  \\n\\n**Suggested Length**: 600–800 words (current length is ~500 words, so additional depth is recommended).  \\n\\nLet me know if you\\'d like help rewriting specific sections!'}}\n",
      "Search failed for query 'Technical architecture and workflow examples of LangChain and LangSmith': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query '(To gather detailed explanations of how each tool functions, including code snippets or pipeline examples.)': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query 'Real-world applications of LangChain and LangSmith in industries like healthcare, finance, or customer service': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query '(To find case studies or examples demonstrating practical use cases for each tool.)': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query 'Integration and synergy between LangChain and LangSmith for end-to-end AI development': can only concatenate str (not \"dict\") to str\n",
      "Search failed for query '(To explore how the tools complement each other in workflows, deployment, and scaling.)': can only concatenate str (not \"dict\") to str\n",
      "{'research_critique': {'content': ['LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', \"LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. You might think of LangSmith as LangChain's counterpart, but it takes things further by focusing on managing, debugging, and orchestrating AI and ML models. LangSmith steps in to give you the tools you need to debug and monitor your models at scale, ensuring everything is running as expected in your AI system. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. If you're debugging complex AI models or managing large-scale workflows with multiple moving parts, LangSmith's advanced debugging and orchestration features will be indispensable.\", '# \"question\": \"What is the difference between LangChain and LangSmith?\",. # \"title\": \"What is LangChain\",. # \"text\": \"Langchain abstracts at a high level', 'for s in graph.stream({. \\'task\\': \"what is the difference between langchain and langsmith\",. \"max_revisions\": 2,. \"revision_number\": 1,. },', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nIn the era of artificial intelligence, the tools that power language models are as vital as the models themselves. Large language models (LLMs) have revolutionized industries, enabling applications from chatbots to data analysis. However, building and deploying these models requires specialized tools. **LangChain** and **LangSmith** are two such platforms, each designed to address distinct stages of AI development. While both facilitate working with LLMs, their focus, functionality, and target users differ significantly.  \\n\\n**LangChain** is a framework centered on the *development* of AI applications. It empowers developers to construct complex workflows by chaining together multiple models, APIs, and data sources. For instance, a developer might use LangChain to create a pipeline that preprocesses user input, feeds it into a language model, and then formats the output for a chatbot. Its emphasis on *flexibility and customization* makes it ideal for engineers and data scientists who need fine-grained control over their AI systems. Features like custom prompts, memory management, and integration with external tools (e.g., databases or analytics platforms) highlight its role in *building* sophisticated applications.  \\n\\nIn contrast, **LangSmith** focuses on the *deployment and management* of AI workflows. Designed for teams and organizations, it streamlines the process of testing, monitoring, and scaling AI applications. For example, a product team might use LangSmith to deploy a chatbot to a cloud environment, track its performance in real time, and debug issues without requiring deep coding expertise. Its user-friendly interface and collaboration tools (e.g., version control and team workflows) cater to non-developers, such as product managers or DevOps teams. By prioritizing *ease of use and scalability*, LangSmith ensures that AI solutions are not only built but also maintained effectively.  \\n\\nThe core difference between the two lies in their purpose: **LangChain is for *building* AI workflows**, while **LangSmith is for *running and refining* them**. LangChain’s strength lies in its open-source flexibility, appealing to developers who prioritize customization. LangSmith, on the other hand, excels in simplifying deployment, making it a choice for teams focused on scalability and collaboration. While LangChain might be used to design a multi-step data analysis pipeline, LangSmith would handle the subsequent steps of testing, monitoring, and refining that pipeline in a production environment.  \\n\\nIn conclusion, LangChain and LangSmith are complementary tools in the AI ecosystem. LangChain’s focus on development enables innovation, while LangSmith ensures that these innovations can be reliably deployed and maintained. For organizations, the choice between the two depends on their needs: developers seeking control and flexibility may favor LangChain, while teams prioritizing collaboration and scalability might lean toward LangSmith. Together, they exemplify how specialized tools can transform the potential of large language models into real-world impact.  \\n\\n---  \\nThis essay adheres to the outlined structure, emphasizing clarity and logical flow. It balances technical details with accessible explanations, ensuring readers grasp the distinct roles of both platforms.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, ValidationError\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.tools import tool\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from IPython.display import Image\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# connect to tavily search tool - use your tavily api key\n",
    "os.environ['TAVILY_API_KEY']=\"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\"\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "#tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "#define agent state\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "#define and configure the model\n",
    "# configure model\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=10.0)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=chat_credentials[\"model_name\"], base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "\n",
    "#define prompts\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert writer tasked with writing a high level outline of an eassy. \\\n",
    "Write such an outline for the user provided topic. Give an outline of eassy along \\\n",
    "with any relevant notes or instructions for the sections.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an eassy assistant tasked with writing excellent 5-paragraph eassys. \\\n",
    "Generate the best eassy possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of ypur previous attempts. \\\n",
    "\n",
    "--------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a teacher grading an eassy submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when writing the following eassy. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as oulined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. \\\n",
    "Only generate 3 queries max.\n",
    "\"\"\"\n",
    "    \n",
    "def extract_json(text):\n",
    "    # Remove unwanted tags like <think> and <speak>\n",
    "    cleaned_text = re.sub(r'<\\/?[\\w\\d]+>', '', text).strip()\n",
    "\n",
    "    # Now try to extract the JSON part using regex\n",
    "    match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "def normalize_to_queries(output: str):\n",
    "    \"\"\"\n",
    "    Convert model output into a dict matching Queries schema.\n",
    "    \"\"\"\n",
    "    # Remove <think>...</think> if present\n",
    "    output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Try strict JSON parse\n",
    "    try:\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: convert markdown/bullets/numbered list into dict\n",
    "        lines = [\n",
    "            re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"')\n",
    "            for line in output.splitlines()\n",
    "            if line.strip()\n",
    "        ]\n",
    "        return {\"queries\": lines}\n",
    "    \n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "#implement nodes\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\"plan\": response_content}\n",
    "    \n",
    "def research_plan_node(state: AgentState):\n",
    "    print(\"entering research_plan_node\")\n",
    "    raw_response = model.invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    response_content = raw_response.content if hasattr(raw_response, 'content') else str(raw_response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    try:\n",
    "        # Try parsing as JSON first\n",
    "        try:\n",
    "            json_data = json.loads(response_content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: convert numbered/bulleted list into dict\n",
    "            lines = [line.strip(\"0123456789. -\") for line in response_content.splitlines() if line.strip()]\n",
    "            json_data = {\"queries\": lines}\n",
    "\n",
    "        print(\"json data in research plan mode:\", json_data)\n",
    "        response_content = normalize_to_queries(response_content)\n",
    "        queries = Queries.model_validate(json_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from model output:\", e)\n",
    "        return {\"content\": state.get('content', [])}\n",
    "\n",
    "    content = state.get('content', [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        print(\"tavily search response:\", response)\n",
    "        for r in response.get('results', []):\n",
    "            content_piece = r.get(\"content\", \"\")\n",
    "            content.append(str(content_piece))\n",
    "    print(\"exiting research_plan_node\")\n",
    "    return {\"content\": content}\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join([\"content\"] or [])\n",
    "    user_message = HumanMessage(content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\n",
    "        \"draft\": response_content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft']),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    return {\"critique\": response_content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    try:\n",
    "        # Run the model\n",
    "        raw_response = model.invoke([\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state['critique'])\n",
    "        ])\n",
    "        # Safely get the content\n",
    "        response_content = raw_response.content if hasattr(raw_response, 'content') else str(raw_response)\n",
    "        \n",
    "\n",
    "        # Remove <think>...</think> blocks completely\n",
    "        response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL)\n",
    "\n",
    "        # Extract JSON from model output (if Qwen adds extra tokens)\n",
    "        response_content = normalize_to_queries(response_content)\n",
    "        queries = Queries.model_validate(response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during model invocation or JSON parsing:\", e)\n",
    "        return {\"content\": state.get(\"content\", [])}\n",
    "\n",
    "    content = state.get(\"content\", [])\n",
    "    for q in queries.queries:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            print(\"tavily response\" + response)\n",
    "            for r in response.get('results', []):\n",
    "                content_piece = r.get(\"content\", \"\")\n",
    "                content.append(str(content_piece))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "    \n",
    "    return {\"content\": content}\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "#build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "#set entry point\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "#define conditional edges\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "#define edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "#print graph\n",
    "#Image(graph.get_graph().draw_png())\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    for s in graph.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba91136-e012-4996-aa8e-409a3f70bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://34c82e2db1fa16a1b0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://34c82e2db1fa16a1b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import ewriter, writer_gui\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a74a3-9c14-41a5-9131-80bd7064f444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
