{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e741c4-bf73-4f95-8265-8e85ecf75e1f",
   "metadata": {},
   "source": [
    "## Langgraph Essay Writer Agent \n",
    "\n",
    "You are an expert writer tasked with writing a high level outline of an essay.  Write such an outline for user provided topic.  Give an outline of the essay along with notes and instructions for the sections\n",
    "\n",
    "##### Note: Go to JupyterLab terminal and execute following command before getting started\n",
    "<pre>\n",
    "    uv add langgraph\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316b0f92-e9cd-49d2-85fe-08a803e29513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': '<think>\\nOkay, the user is asking about the difference between LangChain and LangSmith. I need to create a high-level essay outline that clearly distinguishes these two tools. Let me start by recalling what I know about each.\\n\\nLangChain is a framework for building applications with large language models. It provides tools for data integration, prompt management, and model orchestration. It\\'s more about the development process, allowing users to create chains of operations.\\n\\nLangSmith, on the other hand, is a platform for testing, debugging, and deploying AI applications. It\\'s more focused on the lifecycle management of AI models, offering features like monitoring, version control, and collaboration tools.\\n\\nSo the main points of difference would be their primary functions, target audience, key features, use cases, and integration with other tools. I should structure the essay to first introduce both, then compare them in each of these areas.\\n\\nWait, I should make sure I\\'m not mixing up any details. Let me verify: LangSmith is indeed part of the LangChain ecosystem, right? Or are they separate? I think LangSmith is a newer tool, possibly developed by the same team as LangChain, but with a different focus. So maybe in the outline, I should mention that they are related but serve different purposes.\\n\\nThe user might be a developer or a data scientist trying to choose between the two. They need to understand which tool fits their needs. The essay should highlight that LangChain is for building applications, while LangSmith is for managing and deploying them.\\n\\nI should also think about the structure. Maybe start with an introduction, then a section on each tool, followed by a comparison. But the user asked for an outline, not the full essay. So the outline should have sections like Introduction, LangChain Overview, LangSmith Overview, Key Differences, Use Cases, Conclusion.\\n\\nIn the notes, I should remind the writer to define both tools clearly, provide examples of their features, and explain the practical implications of their differences. Also, make sure to highlight that while they are related, their purposes are distinct.\\n</think>\\n\\n### **Essay Outline: The Difference Between LangChain and LangSmith**\\n\\n---\\n\\n#### **I. Introduction**  \\n- **Purpose**: Introduce the growing importance of AI development tools and platforms.  \\n- **Context**: Briefly mention the rise of LangChain and LangSmith as key tools in the AI/LLM ecosystem.  \\n- **Thesis Statement**: While both LangChain and LangSmith are designed to streamline AI development, they differ significantly in their focus, features, and use cases.  \\n\\n**Notes**:  \\n- Define \"LLM\" (Large Language Model) for clarity.  \\n- Highlight that the essay will compare the two tools to help readers choose the right tool for their needs.  \\n\\n---\\n\\n#### **II. Overview of LangChain**  \\n- **Definition**: LangChain is a framework for building applications with large language models (LLMs).  \\n- **Key Features**:  \\n  - **Chain building**: Enables users to create and manage sequences of operations (e.g., data retrieval, prompt engineering, model execution).  \\n  - **Modular components**: Supports integration with various LLMs, databases, and APIs.  \\n  - **Prompt management**: Tools for optimizing and iterating on prompts.  \\n- **Target Audience**: Developers, data scientists, and AI engineers building custom LLM applications.  \\n- **Use Cases**:  \\n  - Creating chatbots with custom workflows.  \\n  - Building data-driven AI pipelines.  \\n\\n**Notes**:  \\n- Emphasize LangChain’s role in *development* (e.g., coding chains, debugging).  \\n- Mention its open-source nature and community-driven tools.  \\n\\n---\\n\\n#### **III. Overview of LangSmith**  \\n- **Definition**: LangSmith is a platform for testing, debugging, and deploying AI applications.  \\n- **Key Features**:  \\n  - **End-to-end workflow management**: From prototyping to production.  \\n  - **Collaboration tools**: Enables team-based development and version control.  \\n  - **Monitoring and analytics**: Tracks performance, user interactions, and model behavior.  \\n  - **Integration with LLMs**: Supports deployment of AI models across environments.  \\n- **Target Audience**: Teams building scalable AI products, including product managers and DevOps engineers.  \\n- **Use Cases**:  \\n  - Deploying AI models in production.  \\n  - Iterating on user feedback and improving model performance.  \\n\\n**Notes**:  \\n- Highlight LangSmith’s focus on *deployment and lifecycle management*.  \\n- Note its integration with LangChain (e.g., LangSmith as a companion tool for testing chains).  \\n\\n---\\n\\n#### **IV. Key Differences Between LangChain and LangSmith**  \\n- **Primary Focus**:  \\n  - **LangChain**: Development of AI applications (coding chains, prompts, data integration).  \\n  - **LangSmith**: Deployment and management of AI workflows (monitoring, collaboration, scalability).  \\n- **Functionality**:  \\n  - LangChain = “Building the AI.”  \\n  - LangSmith = “Running the AI.”  \\n- **Audience**:  \\n  - LangChain: Developers and engineers.  \\n  - LangSmith: Teams and organizations focused on scaling AI.  \\n- **Integration**:  \\n  - LangSmith often works with LangChain to test and deploy chains.  \\n- **Use Case Examples**:  \\n  - A developer uses LangChain to build a chain for a customer support chatbot.  \\n  - A team uses LangSmith to monitor the chatbot’s performance and update it based on user feedback.  \\n\\n**Notes**:  \\n- Use bullet points or a table for clarity.  \\n- Emphasize that the tools complement each other.  \\n\\n---\\n\\n#### **V. Use Cases and Scenarios**  \\n- **Scenario 1**: A startup building a prototype chatbot.  \\n  - **Tool**: LangChain (for building the chain).  \\n  - **Tool**: LangSmith (for testing and iterating on the prototype).  \\n- **Scenario 2**: A company deploying a customer-facing AI assistant.  \\n  - **Tool**: LangSmith (for monitoring and scaling the assistant).  \\n  - **Tool**: LangChain (for updating the chain with new features).  \\n- **Scenario 3**: A developer focusing on prompt engineering.  \\n  - **Tool**: LangChain (for refining prompts and chains).  \\n\\n**Notes**:  \\n- Highlight that the choice depends on the project stage (development vs. deployment).  \\n- Mention hybrid workflows where both tools are used together.  \\n\\n---\\n\\n#### **VI. Conclusion**  \\n- **Summary**: Recap the core differences: LangChain for *building* AI applications, LangSmith for *deploying* and *managing* them.  \\n- **Implications**: Both tools are essential in the AI development lifecycle but serve distinct purposes.  \\n- **Final Thought**: The choice between LangChain and LangSmith depends on whether the goal is to build, test, or scale AI applications.  \\n\\n**Notes**:  \\n- Encourage readers to evaluate their project needs (e.g., development, deployment, collaboration).  \\n- Mention that the tools are part of a broader ecosystem (e.g., LangChain’s integration with other platforms).  \\n\\n--- \\n\\n### **Instructions for Writing the Essay**  \\n1. **Define terms**: Ensure clarity by defining LangChain and LangSmith early.  \\n2. **Use examples**: Provide concrete examples of each tool’s application.  \\n3. **Compare directly**: Use a dedicated section (e.g., “Key Differences”) to avoid confusion.  \\n4. **Highlight synergy**: Emphasize how the tools can work together (e.g., LangChain for development, LangSmith for deployment).  \\n5. **Avoid jargon**: Explain technical terms (e.g., “chains,” “prompts”) for broader accessibility.'}}\n",
      "Error parsing JSON from model output: No JSON object found in response\n",
      "{'research_plan': {'content': []}}\n",
      "{'generate': {'draft': '<think>\\nOkay, I need to write a 5-paragraph essay comparing LangChain and LangSmith. Let me start by recalling what I know about these tools. \\n\\nFirst, LangChain is a framework for building applications with large language models. It allows developers to create chains of operations, manage prompts, and integrate data sources. LangSmith, on the other hand, is a platform focused on testing, debugging, and deploying AI applications. It provides tools for monitoring, version control, and collaboration.\\n\\nThe user wants a clear distinction between the two. I should structure the essay to first introduce both tools, then compare them in terms of their primary functions, target audiences, key features, and use cases. \\n\\nWait, the user provided an outline, so I need to follow that. The outline has sections: Introduction, LangChain Overview, LangSmith Overview, Key Differences, Use Cases, Conclusion. But the user wants a 5-paragraph essay, so I need to condense the outline into that. \\n\\nThe introduction should briefly mention both tools and state the thesis. Then, each paragraph can cover one of the key differences or aspects. Maybe the first paragraph introduces both, the second covers LangChain, the third LangSmith, the fourth the key differences, and the fifth the conclusion. But the user wants 5 paragraphs, so I need to make sure each paragraph is focused.\\n\\nAlso, the user mentioned that if there\\'s critique, I should revise. But since this is the first attempt, I need to write the essay based on the outline provided. \\n\\nI should start by defining both tools in the introduction. Then, in the next paragraphs, explain each tool\\'s purpose, features, and audience. Then, compare them, and conclude with their roles in the AI development lifecycle. \\n\\nI need to make sure the essay is clear, concise, and highlights the differences without getting too technical. Also, use examples to illustrate their use cases. \\n\\nLet me check if I have all the key points: LangChain is for building applications, LangSmith for testing and deployment. Their target audiences are different, and their features align with those purposes. Use cases would include things like developing a chatbot with LangChain and deploying it with LangSmith. \\n\\nI should avoid jargon and explain terms like \"chains\" or \"prompt management\" if necessary. Also, make sure the conclusion ties it all together, emphasizing that they complement each other.\\n</think>\\n\\n**The Difference Between LangChain and LangSmith**  \\n\\nLangChain and LangSmith are both critical tools in the AI development ecosystem, but they serve distinct purposes. LangChain is a framework designed to help developers build applications using large language models (LLMs). It emphasizes creating chains of operations—such as data retrieval, prompt engineering, and model execution—that allow for complex, customizable workflows. In contrast, LangSmith is a platform focused on testing, debugging, and deploying AI applications. While LangChain is about *building* AI systems, LangSmith is about *refining and scaling* them. Together, they form a comprehensive toolkit for AI development.  \\n\\nLangChain’s strength lies in its modularity and flexibility. Developers can use it to integrate LLMs with databases, APIs, and other tools, enabling the creation of sophisticated applications like chatbots, data analysis systems, and automation workflows. For example, a developer might use LangChain to design a chain that retrieves customer data, generates a personalized response, and sends it via email. This tool is ideal for teams focused on the *development phase*, where the goal is to prototype and iterate on AI functionality.  \\n\\nLangSmith, on the other hand, is tailored for the *deployment and management* of AI applications. It provides features such as real-time monitoring, version control, and collaboration tools, which are essential for teams scaling AI solutions. For instance, a company might use LangSmith to track how a customer support chatbot performs in production, identify bottlenecks, and update the system based on user feedback. Its emphasis on *operational efficiency* makes it indispensable for organizations aiming to maintain and improve AI systems over time.  \\n\\nThe key difference between the two lies in their focus: LangChain is a *development framework*, while LangSmith is a *deployment platform*. LangChain empowers developers to create custom AI workflows, whereas LangSmith ensures these workflows are reliable, scalable, and user-friendly. For example, a startup might use LangChain to build a prototype AI assistant and then leverage LangSmith to monitor its performance and optimize it for millions of users.  \\n\\nIn summary, LangChain and LangSmith are complementary tools that address different stages of AI development. While LangChain is the go-to tool for building and testing AI applications, LangSmith ensures these applications are robust and efficient in real-world scenarios. Understanding their roles helps developers and organizations choose the right tools to innovate, iterate, and scale their AI solutions effectively.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '<think>'}}\n",
      "Error during model invocation or JSON parsing: name 'AIMessage' is not defined\n",
      "{'research_critique': {'content': []}}\n",
      "{'generate': {'draft': '<think>\\nOkay, I need to write a 5-paragraph essay comparing LangChain and LangSmith. Let me start by recalling what I know about these tools. \\n\\nFirst, LangChain is a framework for building applications with large language models. It allows developers to create chains of operations, manage prompts, and integrate data sources. LangSmith, on the other hand, is a platform focused on testing, debugging, and deploying AI applications. It provides tools for monitoring, version control, and collaboration.\\n\\nThe user wants a clear distinction between the two. I should structure the essay to first introduce both tools, then compare them in terms of their primary functions, target audiences, key features, and use cases. \\n\\nWait, the user provided an outline, so I need to follow that. The outline has sections: Introduction, LangChain Overview, LangSmith Overview, Key Differences, Use Cases, Conclusion. But the user wants a 5-paragraph essay, so I need to condense the outline into that. \\n\\nThe introduction should briefly mention both tools and state the thesis. Then, each paragraph can cover one of the key differences or aspects. Maybe the first paragraph introduces both, the second covers LangChain, the third LangSmith, the fourth the key differences, and the fifth the conclusion. But the user wants 5 paragraphs, so I need to make sure each paragraph is focused.\\n\\nAlso, the user mentioned that if there\\'s critique, I should revise. But since this is the first attempt, I need to write the essay based on the outline provided. \\n\\nI should start by defining both tools in the introduction. Then, in the next paragraphs, explain each tool\\'s purpose, features, and audience. Then, compare them, and conclude with their roles in the AI development lifecycle. \\n\\nI need to make sure the essay is clear, concise, and highlights the differences without getting too technical. Also, use examples to illustrate their use cases. \\n\\nLet me check if I have all the key points: LangChain is for building applications, LangSmith for testing and deployment. Their target audiences are different, and their features align with those purposes. Use cases would include things like developing a chatbot with LangChain and deploying it with LangSmith. \\n\\nI should avoid jargon and explain terms like \"chains\" or \"prompt management\" if necessary. Also, make sure the conclusion ties it all together, emphasizing that they complement each other.\\n</think>\\n\\n**The Difference Between LangChain and LangSmith**  \\n\\nLangChain and LangSmith are both critical tools in the AI development ecosystem, but they serve distinct purposes. LangChain is a framework designed to help developers build applications using large language models (LLMs). It emphasizes creating chains of operations—such as data retrieval, prompt engineering, and model execution—that allow for complex, customizable workflows. In contrast, LangSmith is a platform focused on testing, debugging, and deploying AI applications. While LangChain is about *building* AI systems, LangSmith is about *refining and scaling* them. Together, they form a comprehensive toolkit for AI development.  \\n\\nLangChain’s strength lies in its modularity and flexibility. Developers can use it to integrate LLMs with databases, APIs, and other tools, enabling the creation of sophisticated applications like chatbots, data analysis systems, and automation workflows. For example, a developer might use LangChain to design a chain that retrieves customer data, generates a personalized response, and sends it via email. This tool is ideal for teams focused on the *development phase*, where the goal is to prototype and iterate on AI functionality.  \\n\\nLangSmith, on the other hand, is tailored for the *deployment and management* of AI applications. It provides features such as real-time monitoring, version control, and collaboration tools, which are essential for teams scaling AI solutions. For instance, a company might use LangSmith to track how a customer support chatbot performs in production, identify bottlenecks, and update the system based on user feedback. Its emphasis on *operational efficiency* makes it indispensable for organizations aiming to maintain and improve AI systems over time.  \\n\\nThe key difference between the two lies in their focus: LangChain is a *development framework*, while LangSmith is a *deployment platform*. LangChain empowers developers to create custom AI workflows, whereas LangSmith ensures these workflows are reliable, scalable, and user-friendly. For example, a startup might use LangChain to build a prototype AI assistant and then leverage LangSmith to monitor its performance and optimize it for millions of users.  \\n\\nIn summary, LangChain and LangSmith are complementary tools that address different stages of AI development. While LangChain is the go-to tool for building and testing AI applications, LangSmith ensures these applications are robust and efficient in real-world scenarios. Understanding their roles helps developers and organizations choose the right tools to innovate, iterate, and scale their AI solutions effectively.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, ValidationError\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.tools import tool\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from IPython.display import Image\n",
    "import re\n",
    "\n",
    "\n",
    "# connect to tavily search tool - use your tavily api key\n",
    "os.environ['TAVILY_API_KEY']=\"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\"\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "#tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "#define agent state\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "#define and configure the model\n",
    "# configure model\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=10.0)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=chat_credentials[\"model_name\"], base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "\n",
    "#define prompts\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert writer tasked with writing a high level outline of an eassy. \\\n",
    "Write such an outline for the user provided topic. Give an outline of eassy along \\\n",
    "with any relevant notes or instructions for the sections.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an eassy assistant tasked with writing excellent 5-paragraph eassys. \\\n",
    "Generate the best eassy possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of ypur previous attempts. \\\n",
    "\n",
    "--------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a teacher grading an eassy submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when writing the following eassy. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as oulined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. \\\n",
    "Only generate 3 queries max.\n",
    "\"\"\"\n",
    "    \n",
    "def extract_json(text):\n",
    "    # Remove unwanted tags like <think> and <speak>\n",
    "    cleaned_text = re.sub(r'<\\/?[\\w\\d]+>', '', text).strip()\n",
    "\n",
    "    # Now try to extract the JSON part using regex\n",
    "    match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response\")\n",
    "    return json.loads(match.group(0))\n",
    "    \n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "#implement nodes\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "    \n",
    "def research_plan_node(state: AgentState):\n",
    "    raw_response = model.invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        json_data = extract_json(raw_response.content if hasattr(raw_response, 'content') else str(raw_response))\n",
    "        queries = Queries.model_validate(json_data)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from model output:\", e)\n",
    "        return {\"content\": state.get('content', [])}\n",
    "\n",
    "    content = state.get('content', [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "\n",
    "    return {\"content\": content}\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join([\"content\"] or [])\n",
    "    user_message = HumanMessage(content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft']),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    try:\n",
    "        # Run the model\n",
    "        raw_response = model.invoke([\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state['critique'])\n",
    "        ])\n",
    "        \n",
    "        # Extract JSON from model output (if Qwen adds extra tokens)\n",
    "        text_output = raw_response.content if isinstance(raw_response, AIMessage) else str(raw_response)\n",
    "        parsed = extract_json(text_output)\n",
    "        queries = Queries.model_validate(parsed)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during model invocation or JSON parsing:\", e)\n",
    "        return {\"content\": state.get(\"content\", [])}\n",
    "\n",
    "    content = state.get(\"content\", [])\n",
    "    for q in queries.queries:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content.append(r.get(\"content\", \"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "    \n",
    "    return {\"content\": content}\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "#build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "#set entry point\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "#define conditional edges\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "#define edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "#print graph\n",
    "#Image(graph.get_graph().draw_png())\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    for s in graph.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba91136-e012-4996-aa8e-409a3f70bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://ada4c3b827e47d7964.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ada4c3b827e47d7964.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/gradio/blocks.py\", line 2250, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/gradio/blocks.py\", line 1757, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/vcap/app/.venv/lib/python3.13/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/home/vcap/app/cf-jupyterlab-samples/langGraphNotebooks/helper.py\", line 355, in updt_disp\n",
      "    s_thread_ts = state.config['configurable']['thread_ts']\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'thread_ts'\n"
     ]
    }
   ],
   "source": [
    "from helper import ewriter, writer_gui\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a74a3-9c14-41a5-9131-80bd7064f444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
